{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ecd3e50",
   "metadata": {},
   "source": [
    "# Preprocess Dataset: Segmentation and Train, Test, Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21889115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Created 756 total segments.\n",
      "Train: 604, Val: 75, Test: 77\n",
      "Processed dataset saved at: /Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/processed_dataset\n"
     ]
    }
   ],
   "source": [
    "import json, random\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "import kagglehub\n",
    "from preprocess_utils import segment_words\n",
    "import os\n",
    "\n",
    "# --- Disable parallelism for tokenizers to avoid warnings ---\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# --- Download Kaggle dataset ---\n",
    "path = kagglehub.dataset_download(\"etaifour/trump-speeches-audio-and-word-transcription\")\n",
    "\n",
    "# --- Settings ---\n",
    "output_dir = Path(\"processed_dataset\")\n",
    "split_ratios = (0.8, 0.1, 0.1) # train, val, test\n",
    "min_len, max_len = 5.0, 30.0  # max and min length of audio segements in seconds\n",
    "random.seed(42)\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "meta = []\n",
    "dataset_root = Path(path)\n",
    "\n",
    "# --- Process each audio + transcript pair ---\n",
    "for file in [\"Trump_WEF_2018\", \"Trumps_speech_at_75th_d_day_anniversary_in_normandy_full_remarks_UhOMVlQxapY\", \"state of the union 2018\", \"state-of-the-union-trump_2019-02-05-225820-8225-0-0-0.64kmono\"]:\n",
    "    audio_file = dataset_root / f\"{file}.mp3\"\n",
    "    json_file = audio_file.with_name(audio_file.name + \".json\")\n",
    "\n",
    "    if not json_file or not json_file.exists():\n",
    "        print(f\"⚠️ Skipping {audio_file.name}, no matching transcript.\")\n",
    "        continue\n",
    "    audio = AudioSegment.from_file(audio_file)\n",
    "    with open(json_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        transcript = json.load(f)\n",
    "\n",
    "    segments = segment_words(transcript[\"words\"], min_len=5.0, max_len=30.0)\n",
    "\n",
    "    for i, (start, end, text) in enumerate(segments):\n",
    "        clip_name = f\"{audio_file.stem}_{i:04d}.wav\"\n",
    "        clip_path = output_dir / \"audio\" / clip_name\n",
    "        clip_path.parent.mkdir(exist_ok=True)\n",
    "\n",
    "        clip = audio[start * 1000 : end * 1000]\n",
    "        clip.export(clip_path, format=\"wav\", parameters=[\"-ar\", \"16000\", \"-ac\", \"1\"])\n",
    "\n",
    "        meta.append({\n",
    "            \"audio\": f\"audio/{clip_name}\",  # relative path\n",
    "            \"text\": text.strip(),\n",
    "            \"duration\": round(end - start, 3)\n",
    "        })\n",
    "\n",
    "# --- Split into train/val/test ---\n",
    "random.shuffle(meta)\n",
    "n = len(meta)\n",
    "n_train = int(split_ratios[0] * n)\n",
    "n_val = int(split_ratios[1] * n)\n",
    "\n",
    "splits = {\n",
    "    \"train\": meta[:n_train],\n",
    "    \"validation\": meta[n_train:n_train + n_val],\n",
    "    \"test\": meta[n_train + n_val:]\n",
    "}\n",
    "\n",
    "# --- Save to JSONL files ---\n",
    "for split, items in splits.items():\n",
    "    with open(output_dir / f\"{split}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in items:\n",
    "            json.dump(item, f)\n",
    "            f.write(\"\\n\")  # JSONL format\n",
    "\n",
    "print(f\"\\n✅ Created {len(meta)} total segments.\")\n",
    "print(f\"Train: {len(splits['train'])}, Val: {len(splits['validation'])}, Test: {len(splits['test'])}\")\n",
    "print(f\"Processed dataset saved at: {output_dir.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b055351",
   "metadata": {},
   "source": [
    "# Setup Weights and Biases for Metrics, Whisper Processor, Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2183970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lucky-gorge-6</strong> at: <a href='https://wandb.ai/johnfoley2003-university-of-limerick/whisper-fine-tune/runs/3z5w5hbn' target=\"_blank\">https://wandb.ai/johnfoley2003-university-of-limerick/whisper-fine-tune/runs/3z5w5hbn</a><br> View project at: <a href='https://wandb.ai/johnfoley2003-university-of-limerick/whisper-fine-tune' target=\"_blank\">https://wandb.ai/johnfoley2003-university-of-limerick/whisper-fine-tune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251022_134539-3z5w5hbn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/wandb/run-20251022_134855-xlkoeui8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/johnfoley2003-university-of-limerick/whisper-fine-tune/runs/xlkoeui8' target=\"_blank\">azure-lake-7</a></strong> to <a href='https://wandb.ai/johnfoley2003-university-of-limerick/whisper-fine-tune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/johnfoley2003-university-of-limerick/whisper-fine-tune' target=\"_blank\">https://wandb.ai/johnfoley2003-university-of-limerick/whisper-fine-tune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/johnfoley2003-university-of-limerick/whisper-fine-tune/runs/xlkoeui8' target=\"_blank\">https://wandb.ai/johnfoley2003-university-of-limerick/whisper-fine-tune/runs/xlkoeui8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer, GenerationConfig\n",
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "import wandb, torch\n",
    "from jiwer import wer\n",
    "from data_collator import DataCollatorSpeechSeq2SeqWithPadding\n",
    "from prepare_dataset import AudioTextDataset\n",
    "\n",
    "# Load the processor for feature extraction and tokenization\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "\n",
    "# Initialize the data collator to pad variable-length audio/text inputs within a batch\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "# Initialize Weights & Biases for experiment tracking\n",
    "wandb.init(\n",
    "    project=\"whisper-fine-tune\",  # Name of the project on wandb\n",
    ")\n",
    "\n",
    "# --- Decoder settings ---\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\")\n",
    "decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids(\"<|startoftranscript|>\")\n",
    "\n",
    "# --- Metrics computation (Word Error Rate) ---\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    preds = processor.batch_decode(pred.predictions, skip_special_tokens=True)\n",
    "    refs = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    return {\"wer\": wer(refs, preds)}\n",
    "\n",
    "# --- Datasets ---\n",
    "train_dataset = AudioTextDataset(json_path=\"processed_dataset/train.json\", processor=processor)\n",
    "val_dataset   = AudioTextDataset(json_path=\"processed_dataset/validation.json\", processor=processor)\n",
    "test_dataset  = AudioTextDataset(json_path=\"processed_dataset/test.json\", processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ad061b",
   "metadata": {},
   "source": [
    "# Full Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02389b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 01:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FULL] Before fine-tuning → Loss: 2.6783, WER: 0.226\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='608' max='608' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [608/608 12:53, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.184400</td>\n",
       "      <td>2.031796</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.254913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.045500</td>\n",
       "      <td>1.253105</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.230208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.909400</td>\n",
       "      <td>1.111961</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.267827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.762000</td>\n",
       "      <td>1.027179</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.262774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.662200</td>\n",
       "      <td>0.936654</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.262212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.355500</td>\n",
       "      <td>0.698043</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.236384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.316000</td>\n",
       "      <td>0.687892</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.229085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.285800</td>\n",
       "      <td>0.685406</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.232454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.202300</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.233015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.274900</td>\n",
       "      <td>0.684588</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.273442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.233100</td>\n",
       "      <td>0.685400</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.233015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.262800</td>\n",
       "      <td>0.686500</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.233015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3922: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n",
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FULL] Model and processor saved to models/fine_tuned_whisper_full_20251022-140211\n"
     ]
    }
   ],
   "source": [
    "# Full fine-tuning\n",
    "\n",
    "learning_rate = 2e-5\n",
    "warmup_steps = 50\n",
    "\n",
    "training_args_full = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"checkpoints/full\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_steps=25,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    num_train_epochs=8,   # reduce epochs for small dataset\n",
    "    learning_rate=learning_rate,\n",
    "    warmup_steps=warmup_steps,\n",
    "    save_total_limit=2,\n",
    "    report_to=[\"wandb\"],  # Log metrics to Weights & Biases\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_wer\",\n",
    "    greater_is_better=False,  # lower WER is better\n",
    "    generation_max_length=128,\n",
    "    max_grad_norm=1.0,\n",
    " )\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"  # MPS (macOS) or CPU\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "\n",
    "# Unfreeze all model parameters for full fine-tuning\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Configure generation and caching\n",
    "model.config.use_cache = False\n",
    "model.config.forced_decoder_ids = forced_decoder_ids\n",
    "model.config.decoder_start_token_id = decoder_start_token_id\n",
    "\n",
    "trainer_full = Seq2SeqTrainer(\n",
    "    args=training_args_full,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    " )\n",
    "\n",
    "# --- Evaluate before training ---\n",
    "test_pre_eval_full = trainer_full.evaluate(eval_dataset=test_dataset)\n",
    "print(f\"[FULL] Before fine-tuning → Test Loss: {test_pre_eval_full['eval_loss']:.4f}, WER: {test_pre_eval_full['eval_wer']:.3f}\")\n",
    "val_pre_eval_full = trainer_full.evaluate(eval_dataset=val_dataset)\n",
    "print(f\"[FULL] Before fine-tuning → Val Loss: {val_pre_eval_full['eval_loss']:.4f}, WER: {val_pre_eval_full['eval_wer']:.3f}\")\n",
    "\n",
    "# --- Training ---\n",
    "trainer_full.train()\n",
    "\n",
    "# === Save the model after training ===\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "models_root = Path(\"models\"); models_root.mkdir(exist_ok=True)\n",
    "full_model_dir = models_root / f\"fine_tuned_whisper_full_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "full_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "model.save_pretrained(full_model_dir)\n",
    "processor.save_pretrained(full_model_dir)\n",
    "print(f\"[FULL] Model and processor saved to {full_model_dir}\")\n",
    "\n",
    "# Keep in memory for the final comparison\n",
    "full_model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dd9b50",
   "metadata": {},
   "source": [
    "# Low Rank Adaptation Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7f675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 01:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LoRA] Before fine-tuning → Loss: 2.6783, WER: 0.226\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='608' max='608' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [608/608 11:25, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.217200</td>\n",
       "      <td>2.044517</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.271196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.124300</td>\n",
       "      <td>1.324261</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.272319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.043000</td>\n",
       "      <td>1.237145</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.244806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.984000</td>\n",
       "      <td>1.198608</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.237507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.970500</td>\n",
       "      <td>1.167904</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.236384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.844900</td>\n",
       "      <td>1.136174</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.236384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.866400</td>\n",
       "      <td>1.094966</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.238630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.779000</td>\n",
       "      <td>0.996046</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.238069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.403900</td>\n",
       "      <td>0.736219</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.239191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.461100</td>\n",
       "      <td>0.717368</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.242560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.418500</td>\n",
       "      <td>0.713877</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.242560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.449400</td>\n",
       "      <td>0.713108</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.243122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LoRA] Model and processor saved to models/fine_tuned_whisper_lora_20251022-141355\n"
     ]
    }
   ],
   "source": [
    "# LoRA fine-tuning\n",
    "\n",
    "learning_rate = 4e-5\n",
    "warmup_steps = 50\n",
    "\n",
    "training_args_lora = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"checkpoints/lora\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_steps=25,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    num_train_epochs=8,   # reduce epochs for small dataset\n",
    "    learning_rate=learning_rate,\n",
    "    warmup_steps=warmup_steps,\n",
    "    save_total_limit=2,\n",
    "    report_to=[\"wandb\"],  # Log metrics to Weights & Biases\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_wer\",\n",
    "    greater_is_better=False,  # lower WER is better\n",
    "    generation_max_length=128,\n",
    "    max_grad_norm=1.0,\n",
    " )\n",
    "\n",
    "# Start from a fresh base model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "\n",
    "# Prepare the model for LoRA-compatible k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA (Low-Rank Adaptation) for efficient fine-tuning\n",
    "config = LoraConfig(\n",
    "    r=32,  # Rank of LoRA decomposition\n",
    "    lora_alpha=64,  # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Apply LoRA to attention projections\n",
    "    lora_dropout=0.05,  # Dropout applied to LoRA layers\n",
    "    bias=\"none\"  # Don't adapt bias terms\n",
    ")\n",
    "\n",
    "# Wrap the base model with LoRA\n",
    "model = get_peft_model(model, config)\n",
    "model.config.use_cache = False  # Disable caching during training\n",
    "model.config.forced_decoder_ids = forced_decoder_ids\n",
    "model.config.decoder_start_token_id = decoder_start_token_id\n",
    "\n",
    "trainer_lora = Seq2SeqTrainer(\n",
    "    args=training_args_lora,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    " )\n",
    "\n",
    "# --- Evaluate before training ---\n",
    "test_pre_eval_lora = trainer_lora.evaluate(eval_dataset=test_dataset)\n",
    "print(f\"[LoRA] Before fine-tuning → Test Loss: {test_pre_eval_lora['eval_loss']:.4f}, WER: {test_pre_eval_lora['eval_wer']:.3f}\")\n",
    "val_pre_eval_lora = trainer_lora.evaluate(eval_dataset=val_dataset)\n",
    "print(f\"[LoRA] Before fine-tuning → Val Loss: {val_pre_eval_lora['eval_loss']:.4f}, WER: {val_pre_eval_lora['eval_wer']:.3f}\")\n",
    "\n",
    "# --- Training ---\n",
    "trainer_lora.train()\n",
    "\n",
    "# === Save the model after training ===\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "models_root = Path(\"models\"); models_root.mkdir(exist_ok=True)\n",
    "lora_model_dir = models_root / f\"fine_tuned_whisper_lora_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "lora_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "model.save_pretrained(lora_model_dir)\n",
    "processor.save_pretrained(lora_model_dir)\n",
    "print(f\"[LoRA] Model and processor saved to {lora_model_dir}\")\n",
    "\n",
    "# Keep in memory for the final comparison\n",
    "lora_model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a180323",
   "metadata": {},
   "source": [
    "# Test Resultant Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff0f9a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/johnm/ISE/AdvancedAI/AI/fine-tune-whisper/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FULL] After fine-tuning → Test Loss: 0.9711, WER: 0.174\n",
      "[FULL] After fine-tuning → Val Loss: 1.2531, WER: 0.230\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LoRA] After fine-tuning → Test Loss: 0.9189, WER: 0.177\n",
      "[LoRA] After fine-tuning → Val Loss: 1.1986, WER: 0.238\n"
     ]
    }
   ],
   "source": [
    "# Evaluate both models on the test set\n",
    "\n",
    "# Evaluation-only arguments\n",
    "eval_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"checkpoints/eval\",\n",
    "    per_device_eval_batch_size=4,\n",
    "    predict_with_generate=True,\n",
    "    report_to=[],  # avoid logging evals\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    " )\n",
    "\n",
    "# Full model evaluation\n",
    "trainer_full_eval = Seq2SeqTrainer(\n",
    "    args=eval_args,\n",
    "    model=full_model,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    " )\n",
    "test_res_full = trainer_full_eval.evaluate(eval_dataset=test_dataset)\n",
    "print(f\"[FULL] After fine-tuning → Test Loss: {test_res_full['eval_loss']:.4f}, WER: {test_res_full['eval_wer']:.3f}\")\n",
    "val_res_full = trainer_full_eval.evaluate(eval_dataset=val_dataset)\n",
    "print(f\"[FULL] After fine-tuning → Val Loss: {val_res_full['eval_loss']:.4f}, WER: {val_res_full['eval_wer']:.3f}\")\n",
    "\n",
    "# LoRA model evaluation\n",
    "trainer_lora_eval = Seq2SeqTrainer(\n",
    "    args=eval_args,\n",
    "    model=lora_model,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    " )\n",
    "test_res_lora = trainer_lora_eval.evaluate(eval_dataset=test_dataset)\n",
    "print(f\"[LoRA] After fine-tuning → Test Loss: {test_res_lora['eval_loss']:.4f}, WER: {test_res_lora['eval_wer']:.3f}\")\n",
    "val_res_lora = trainer_lora_eval.evaluate(eval_dataset=val_dataset)\n",
    "print(f\"[LoRA] After fine-tuning → Val Loss: {val_res_lora['eval_loss']:.4f}, WER: {val_res_lora['eval_wer']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
